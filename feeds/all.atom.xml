<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Till Keyling's Homepage</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2015-11-27T00:00:00+01:00</updated><entry><title>A workaround for Twitter's Search-API limitations: Using the Twitter Websearch and Facepager</title><link href="/a-workaround-for-twitters-search-api-limitations-using-the-twitter-websearch-and-facepager.html" rel="alternate"></link><updated>2015-11-27T00:00:00+01:00</updated><author><name>Till Keyling</name></author><id>tag:,2015-11-27:a-workaround-for-twitters-search-api-limitations-using-the-twitter-websearch-and-facepager.html</id><summary type="html">&lt;p&gt;While API's in general enrich and simplify the process of (automated) data collection (not just for business cases, but for scientific purposes as
well), these structured and well-defined data access-points have some drawbacks. One of them is the access to 'historical' data, that might be 
restricted, as in the case of the Twitter restful Search-API. The &lt;a href="https://dev.twitter.com/rest/public/search"&gt;documentation&lt;/a&gt; states: &lt;/p&gt;
&lt;p&gt;&lt;em&gt;"Also note that the search results at twitter.com may return historical results while the Search API usually only serves tweets from 
the past week."&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Thus, most sample-strategies that rest upon hash tag-querries are restricted (assuming no other (paid) data provider like GNIP etc. is used) 
to contemporary Tweets no older than ~1 week. Especially student's research projects without any  financial resources at hand might fail due to these restrictions, 
because these projects do often rely on historical hash tags analysis (wildly inferred from the incoming questions regarding the Facpager-Tool).&lt;/p&gt;
&lt;p&gt;There is, however, a simple method to access Tweets older than 1 Week (Note: this limitation concerns only the #-Search via the REST-API!):
The &lt;a href="https://twitter.com/search-advanced"&gt;Twitter-Websearch&lt;/a&gt; does not underlie these restriction and provides access to older/historical Tweets.
Especially the "Advanced-Search" has some nice options to specify the time-frame etc., so that it's a really helpfull interface for (scientific) analyses.
While there are lesser restrictions upon the Websearch, it lacks the option to save the Tweet-data in a structured form like the API-variant (using Facepager or whatever tool) provides.
Due to the fact that the target audience for the Facepager has little or no programming knowledge, programming a WebScraper is usually not an option. 
There might be some Scraper out there (see &lt;a href="http://idatassist.com/20-minutes-to-scraping-twitter-for-building-target-lists-no-coding/"&gt;1&lt;/a&gt;,&lt;a href="http://sysnucleus-blog.com/2014/07/15/how-to-scrape-tweets-twitter-data-scraping-using-webharvy/"&gt;2&lt;/a&gt;,&lt;a href="http://sysnucleus-blog.com/2014/07/15/how-to-scrape-tweets-twitter-data-scraping-using-webharvy/"&gt;3&lt;/a&gt;)
, but the easiest way to combine the power of the Websearch and well-defined data-structure of the API is the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Define your Websearch.&lt;/strong&gt; &lt;br /&gt;
    I'd suggest using the "Advanced" Option.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="Advanced Search Twitter" src="/images/AdvSearch.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Restrict:&lt;/strong&gt;&lt;br /&gt;
Select "Live" or restrict the Search-Output to Tweets, assuming that you're only interested in Tweets (and not Accounts or Photos etc.)  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Paginate:&lt;/strong&gt;&lt;br /&gt;
You can scroll through these results. Depending on the amount of Tweets, new ones will load once you reach the end of the page.  &lt;/p&gt;
&lt;p&gt;To facilitate the tedious task of scrolling through the result, use this bookmarklet (drag &amp;amp; drop to your bookmark-bar, click on it while on the
   results-page and repeat the process)  &lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a class='bml' href='javascript: $("html, body").animate({ scrollTop: $(document).height()-$(window).height() });')&gt; Paginate Results Bookmarklet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Collect ID's:&lt;/strong&gt; &lt;br /&gt;
If you are done collecting results, use the second bookmarklet. This one will open a new window (be sure to enable pop-ups!) that contains the ID of the Tweets.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a class 'bml' href='javascript: window.open("data:data:attachment/csv," + encodeURIComponent($.map($(".js-stream-item"), function (i) {     return $(i).attr("data-item-id"); }).join("\n")),"neu.csv");'&gt; Extract Tweets &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Copy &amp;amp; paste these ID's. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Facepager:&lt;/strong&gt;&lt;br /&gt;
   In this step, add the ID's by clicking the "Add nodes"-button and paste the ID's into the window.
   You should see the ID's as new nodes in the main window of the Facepager. 
   Up to this point, no Tweet data (except the ID's itself, of course) has been collected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. Collect data:&lt;/strong&gt; &lt;br /&gt;
   Use a Facepager-Setup to collect Tweets. It's quite likely that you want to use the following settings:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Facepager Settings" src="/images/Facepager_Settings.png" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Collect the data (you can select multiple or all ID's at once)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Some notes and further details:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is tested in Chrome (46.0.2490.8), but it should work in Firefox as well&lt;/li&gt;
&lt;li&gt;Yes, one can automate the pagination process without clicking the Bookmarklet a couple of times. I do not consider this a good practice&lt;/li&gt;
&lt;li&gt;This type of data collection (the Websearch-Scraping part in steps 1-3) might not be supported/intended by Twitter.
  Be sure to reduce your results using the search features and do not simply collect a vast amount of data.&lt;/li&gt;
&lt;li&gt;This is a quick&amp;amp;dirty solution to the problem - feel free to improve it!&lt;/li&gt;
&lt;/ul&gt;</summary><category term="Facepager"></category><category term="Twitter"></category><category term="JavaScript"></category></entry><entry><title>Custom-Domains and GitHub-Pages: Configuration for checkdomain.de</title><link href="/customdomains.html" rel="alternate"></link><updated>2015-09-21T00:00:00+02:00</updated><author><name>Till Keyling</name></author><id>tag:,2015-09-21:customdomains.html</id><summary type="html">&lt;p&gt;Recently, I rebuilt my homepage using the great &lt;a href="https://github.com/getpelican/pelican"&gt;Pelican&lt;/a&gt;-Framework together with GitHub's Page Hosting capabilities.&lt;/p&gt;
&lt;p&gt;While building and hosting a website as a GitHub User-Page is pretty straightforward, I struggled with the setup of a custom-domain using my my domain-provider &lt;a href="http://www.checkdomain.de"&gt;checkdomain.de&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Based on some helpfull &lt;a href="http://stackoverflow.com/questions/9082499/custom-domain-for-github-project-pages/22374542#22374542"&gt;SO-Threads&lt;/a&gt;, I finally managed to set up a custom-domain for the blog.&lt;/p&gt;
&lt;p&gt;As checkdomain.de doesn't really follow the usual terms and labels in their user-interface, creating the respective A-records (in case you use an apex domain like tillkeyling.com!) wasn't that obvious. Thus, here is a screenshot of the configuration for checkdomain.de:&lt;/p&gt;
&lt;h2&gt;Checkdomain.de-Setup&lt;/h2&gt;
&lt;p&gt;There are basically three steps involved to use a custom (apex) domain for your GitHub User-Page:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a CNAME file in the Master-Branch of your GitHub-Repository. This file should contain your custom domain, without the scheme (&lt;em&gt;http://&lt;/em&gt;) and will cause GitHub to serve your content to that domain (at least as I understood it, having no or just a very basic background in Hosting/DNS-related stuff).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the checkdomain user-interface, adjust the IPv4-Adress to GitHub's current &lt;a href="https://help.github.com/articles/tips-for-configuring-an-a-record-with-your-dns-provider/"&gt;IP-Address&lt;/a&gt;, in this case:
    &lt;em&gt;192.30.252.153&lt;/em&gt;
    &lt;em&gt;192.30.252.154&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Checkdomain.de Configuration" src="/images/checkdomain_config.PNG" /&gt;
&lt;br&gt;
&lt;em&gt;Note: I only used a single A-Record via "Allgemeine Einstellungen". Entering the other IP in the DNS-Table below as an A-Record seems to create another redirect. The settings in the checkdomain user-interface are quite confusing here, f.e. no root-apex (@) can be specified. There is no further description available.&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Enter a CNAME-Record at checkdomain for the "www"-subdomain. The target should be &lt;em&gt;username&lt;/em&gt;.github.io. (note the trailing slash!)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You might check the redirects and DNS settings via the &lt;em&gt;dig&lt;/em&gt;-command, or use this &lt;a href="http://www.digwebinterface.com/?hostnames=tillkeyling.com%0D%0Awww.tillkeyling.com&amp;amp;type=&amp;amp;useresolver=8.8.4.4&amp;amp;ns=all&amp;amp;nameservers="&gt;online-tool&lt;/a&gt; to test the correct settings. Updating the records may take a while, and be sure to reset Chrome's DNS and Website-Cache to test your settings.&lt;/p&gt;</summary><category term="GitHub"></category><category term="Hosting"></category></entry><entry><title>Facepager - What it is, what it's not</title><link href="/facepager-what-it-is-what-its-not.html" rel="alternate"></link><updated>2015-09-01T00:00:00+02:00</updated><author><name>Till Keyling</name></author><id>tag:,2015-09-01:facepager-what-it-is-what-its-not.html</id><summary type="html">&lt;p&gt;To our delight ("we", that is Jakob Jünger and Till Keyling), Facepager became a quite popular tool for the collection of data from API's (and some basic Web-Scraping capabilities) - albeit it's name (which is based on a very early version I coded for a student of mine) for any kind of API, although most people may use it to gather data from Facebook and Twitter.
Besides providing the executable and the source-code itself, users may ask for help on a Facebook-Page and/or the GitHub-Repo. These two feedback-mechanism are used quite regularly, while the most user prefer to contact us via E-Mail.
The constantly increasing user-base and thus the increasing number of questions/appeals for help, however, become more and more time-consuming, i.e. a large proportion of my time spent for the tool is allocated answering questions instead of developing/improving the tool itself. While some questions, hints and discussion are really fruitful to increase the usability and capability of the Facepager, a quite significant share is not. IMO, this is caused by a somewhat "fuzzy" scope of the tool, i.e. users are unclear whether the tool is or is not appropriate for their specific task. Thus, here is a quick overview from our perspective regarding the scope, area of use of our software and the intentions behind the development of the Facepager - in short: What it is and what it's not.  &lt;/p&gt;
&lt;p&gt;What it is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a tool to simplify the process of gathering data from JSON-based APIs without the use of programming languages or predefined scripts, while leaving large degrees-of-freedom to the user. Thus, we do not restrict any API-endpoints and allow "useless" requests&lt;/li&gt;
&lt;li&gt;a tool to support the step of "data collection" on a low level&lt;/li&gt;
&lt;li&gt;a tool to document the process of data collection, i.e. errors occurring in the process (on both the side of the API and locally, f.e. ill-defined requests) &lt;/li&gt;
&lt;li&gt;a tool that targets researchers/scientific purposes, rather than other audiences like market researchers or other commercial uses &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What it's not:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a tool to process or analyze data from JSON-based API. This includes filtering the data (this excludes restrictions or filters provided by the API,f.e. limiting the number of results), aggregating the data (again, the are some methods to aggregate data via the API, f.e. the total count of tweets) or plotting the data. In the end, Facepager is just a "layer" to communicate with the API  - it won't tell you what to talk about (API-endpoints), but enables you to talk to the API. &lt;/li&gt;
&lt;li&gt;a "one click"-tool; although one can use the predefined scripts, we encourage users to formulate their needs in terms of the data they want to collect. Thus, users need to deal with the respective API, it's endpoint, structure and limitations... there is an there will be no "collect-all-posts-from-user-xy"-button!&lt;/li&gt;
&lt;li&gt;a tool to avoid certain API-restrictions, f.e. to exclude personal information that is not available via the respective API.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More explicit hints if you ask some questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We won't answer API-specific questions any more, please read the Documentations of the API. Examples: "How do I collect information about all of my Facebook-Friends"?, "How many Tweets will the Twitter-API provide?"&lt;/li&gt;
&lt;li&gt;Read the internal help - it covers some basic, exemplary cases&lt;/li&gt;
&lt;li&gt;If you ask for help, PLEASE specify your version of the Facepager (f.e. 3.6), your operating system ,provide a quick overview of your task and settings (f.e. a screenshot) and the log-files (via PM, do not share the log-files in public, as they may contain personal information. Delete tokens from the log-files!)&lt;/li&gt;
&lt;li&gt;Use forums like stackoverflow.com --&amp;gt; There are tons of API-related questions that can be easily transfered to the Facepager&lt;/li&gt;
&lt;/ul&gt;</summary><category term="Facepager"></category></entry><entry><title>Facepager and the YouTube API v3 - a quick tutorial</title><link href="/facepager-and-the-youtube-api-v3-a-quick-tutorial.html" rel="alternate"></link><updated>2015-06-01T00:00:00+02:00</updated><author><name>Till Keyling</name></author><id>tag:,2015-06-01:facepager-and-the-youtube-api-v3-a-quick-tutorial.html</id><summary type="html">&lt;p&gt;Almost two years ago, I collected lots of information from the YouTube-API (+scraping Referrer-Data) for my PhD-Thesis (basically mapping the view-dynamic for political videos on YouTube on an individual basis over time). While the scale and design of this data-collection was beyond Facapagers capability, the tools itself is well suited for generic API-requests, such as to the YouTube-API (or reddit, f.e.). 
In the meantime, the YouTube-API underwent significant changes: Besides a necessary "upgrade" to JSON-based response bodies (instead of a somewhat obscure XML-Format in v2), there is now a precise quota-system and error-handling as well. Although some endpoints have been closed, the API and it's documentation improved a lot when compared to it preceding versions. &lt;/p&gt;
&lt;p&gt;Recently, Bernhard Rieder developed the &lt;a href="http://thepoliticsofsystems.net/2015/05/exploring-youtube/"&gt;YouTube Data Tools&lt;/a&gt;, a great approach and furthermore open sourced/public available (although I strongly disagree that YouTube is understudied; this might be the case in the social science due to the preference for textual rather than visual entities (see &lt;a href="http://firstmonday.org/ojs/index.php/fm/article/view/4878/3755"&gt;Vis, 2013&lt;/a&gt;), but there is a long research tradition in the information systems and alike when it comes to YouTube). Bernhard’s new tool reminded me to update the YouTube-Presets for the Facepager, enabling it to communicate with the most recent API-version. While Bernhard’s YTDT focusses on some specific analysis task and is easier to use, the approach of the Facepager is more “low-level”, yet probably more generic (I’ll explain the “low-level”-philosophy of our tools in a follow-up post).&lt;/p&gt;
&lt;p&gt;Because there is no generic OAuth-Dialog in the Facepager yet (it's on the to-do list), some steps are necessary before you can start fetching data from the API:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Obtain an API-Key: The v3-API is OAuth-based, by default. OAuth is a secure, yet &lt;a href="http://img.izifunny.com/pics/2013/20130211/original/izifunny-gifdump-feb-12-2013-25-gifs_3.gif"&gt;difficult&lt;/a&gt; authentication procedure that involves multiple "question-and-answer"-steps, basically to prove the existence and credibility of a user (or an app etc.). As long as you don't need to access private API-endpoints (i.e. your own YouTube-account or an account on behalf of others) and only want to obtain public available information, you should stick to the public API access key that Google provides to every developer). &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To get such a key, you must register as a &lt;a href="https://developers.google.com/"&gt;Google Developer&lt;/a&gt; and create an app/project that uses the YouTube-API. See the &lt;a href="https://console.developers.google.com/"&gt;Developer Console&lt;/a&gt; for more information about how to create an project, it's simple and straightforward. &lt;em&gt;Note&lt;/em&gt;: You need to activate/enable the YouTube-API before you generate a browser key (a server key would work as well and is more appropriate, leave referrers/allowed IP-adresses blank)
&lt;img alt="Enable YouTube-API" src="/images/enable_youtube_api.PNG" /&gt; 
&lt;img alt="Generate Server/Browser Key" src="/images/create_key.PNG" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once you created an app and activated the YouTube-API for that app, you have to obtain the public API-access key to authenticate your requests with the Facepager (as you would do if you login with your Facebook or Twitter-credentials). The key is located in the "credentials"-section:
&lt;img alt="API Key location" src="/images/api_key.jpg" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the public API access key&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load the "YouTube API v3" presets in the Facepager &amp;amp; paste the key in the parameter field (as you might guess, as a value of the "key"-parameter, "XY" by default). See the screenshot below, esp. the critical "parameters"-section in the lower left part of the tool.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You're now ready to start gathering data using the YouTube-API. The current preset collects information about a video clip (&lt;a href="https://developers.google.com/youtube/v3/docs/videos/list"&gt;"list"-method&lt;/a&gt;), namely it's title, author etc. ("snippet"-part) and the typical usage-statistics ("statistics"-part). The only information you need to provide is the ID of a YouTube-Video ("https://www.youtube.com?v=HEREISTHEID" ; paste it as a new node using the"Add Node"-Button, you may copy multiples ID's in here as well).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note that each request and each part withdraw "units" from your API-quota (there is a handy &lt;a href="https://developers.google.com/youtube/v3/determine_quota_cost"&gt;quota-calculator&lt;/a&gt;), in this case 5 units from your daily free-account of 50.000.000 units. You might check your quotas in the aforementioned Developer Console, it will decrease for every additional request you make. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is much more information available via the API - check out either the &lt;a href="https://developers.google.com/apis-explorer/#p/youtube/v3/"&gt;API-Explorer&lt;/a&gt; or the Reference-Guide in the documentation of the v3-API. For example, you could retrieve comments via the "commentThreads"-endpoint (in this case, specify a "videoId"-parameter!). 
&lt;img alt="Statistics and Comments" src="/images/comments_youtube.PNG" /&gt;. &lt;/p&gt;
&lt;p&gt;The API provides really interesting information, f.e. one might map the location of a video (or the recording, usually tagged via GPS). Although such metadata is only available for a small share of YouTube-clips, it provides additional insights on how users use YouTube - f.e. clips uploaded in the german "news &amp;amp; politics"-section of the platform deal with the conflicts in the middle east quite often, which can be seen not only by analyzing the clips itself, but also by mapping their geolocation. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Geolocation YouTube" src="/images/geolocation_germany.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;Have fun playing with the API!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOWNLOAD&lt;/strong&gt; &lt;a href="https://gist.githubusercontent.com/dorvak/fb5b54296084512684f6/raw/d78bc5bb75f86a4867459e135d33b7466e710681/YouTube_API_v3_Get_Video_Statistics-3_4"&gt;YouTube API v3 Preset&lt;/a&gt; (see Help-Section 5 for more information about the presets)&lt;/p&gt;</summary></entry><entry><title>Closures in R and Python</title><link href="/closures-in-r-and-python.html" rel="alternate"></link><updated>2015-04-05T00:00:00+02:00</updated><author><name>Till Keyling</name></author><id>tag:,2015-04-05:closures-in-r-and-python.html</id><summary type="html">&lt;p&gt;Seems like my "intermediate" Python knowledge benefits from (re-)learning R again with this &lt;a href="https://www.coursera.org/course/compdata"&gt;Coursera-MOOC&lt;/a&gt;. Stumbling over closures from Wes McKinney's &lt;a href="http://shop.oreilly.com/product/0636920023784.do"&gt;"Python for Data Analysis"&lt;/a&gt;-Book, here is a short comparison of closures in R and Python, respectively:&lt;/p&gt;
&lt;h1&gt;Closures in Python&lt;/h1&gt;
&lt;p&gt;In Python, closures are function that return function (dynamically), while preserving/accessing the namespace of the "caller"-function and having access to this "outer" function enviroment (&lt;a href="http://en.wikipedia.org/wiki/Closure_(computer_science)"&gt;"referencing enviroment"&lt;/a&gt;)&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;#Set &amp;#39;n&amp;#39; as the power and create the n**power-function&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;power_by_n&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c"&gt;#this function has access to the &amp;#39;make_power&amp;#39;namespace and arguments --&amp;gt; n&lt;/span&gt;
        &lt;span class="c"&gt;#Level the base by the power of &amp;#39;n&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="c"&gt;#return the custom-power function&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;power_by_n&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;With this &lt;em&gt;closure&lt;/em&gt;, one could use it as like this:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;cubic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;square&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;cubic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;#--&amp;gt;1000&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;#--&amp;gt;100&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;One could inspect the closure-functions, f.e. via it's attributes:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;cubic&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;func_closure&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cell_contents&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;h1&gt;Closures in R&lt;/h1&gt;
&lt;p&gt;Similar to python (of course with more curly brackets), closures are quite simple in R.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8
9&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;make.power &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;n&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            pow &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    x&lt;span class="o"&gt;^&lt;/span&gt;n
                 &lt;span class="p"&gt;}&lt;/span&gt;
            pow
            &lt;span class="p"&gt;}&lt;/span&gt;

cube &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; make.power&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
square &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; make.power&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Again, inspection is easy using the 'enviroment','ls' and 'get'-functions.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kp"&gt;ls&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kp"&gt;environment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;cube&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Although beeinf very similar, one should keep possible different scoping rules in R and Python in mind. As a first guess and because they are both using lexical scoping, they do not seem to differ that much (but I might be wrong!!):&lt;/p&gt;
&lt;p&gt;In R, this would throw an 'undefined'-error (as well as in Python; the R code is used from the Coursera course mentioned above):&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;f &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            y &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
            y&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; g&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
g &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            x&lt;span class="o"&gt;*&lt;/span&gt;y
                &lt;span class="p"&gt;}&lt;/span&gt;
f&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;This &lt;strong&gt;lexical-scoping&lt;/strong&gt; is used by python as well (maybe it's not pure lexical scoping, but others should argue wether Python is call-by-reference or whatever, has scoping &lt;em&gt;xy&lt;/em&gt; and so on; &lt;a href="https://www.inkling.com/read/learning-python-mark-lutz-4th/chapter-17/python-scope-basics"&gt;this&lt;/a&gt; is a handy &amp;amp; short explanation of Python scoping rules; and &lt;a href="http://adv-r.had.co.nz/"&gt;that&lt;/a&gt; might be worth reading as well.&lt;/p&gt;</summary><category term="Python"></category><category term="R"></category><category term="Programming"></category></entry><entry><title>Twitter "Diversity" Dataset and Python's Pandas Time Series Introduction</title><link href="/twitter-diversity-dataset-and-pythons-pandas-time-series-introduction.html" rel="alternate"></link><updated>2014-05-06T00:00:00+02:00</updated><author><name>Till Keyling</name></author><id>tag:,2014-05-06:twitter-diversity-dataset-and-pythons-pandas-time-series-introduction.html</id><summary type="html">&lt;p&gt;This is just a short introduction/how-to to time-series analysis with open-data. The &lt;em&gt;twitter-diversity&lt;/em&gt; dataset is available  &lt;a href="https://github.com/trifle/twitter-diversity"&gt;here&lt;/a&gt;. Clone it into your folder, &lt;code&gt;cd&lt;/code&gt; into it and start this IPython notebook. Pytho's Pandas-Module, Matplotlib and NumPy are necessary imports. &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to @pascal and @ajungherr making the data available. Read and replicate the corresponding &lt;a href="http://andreasjungherr.net/2013/08/22/new-publication-forecasting-the-pulse-how-deviations-from-regular-patterns-in-online-data-can-identify-offline-phenomena/"&gt;paper&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Imports&lt;/h2&gt;
&lt;p&gt;First of all, import pandas and activate the &lt;code&gt;pylab&lt;/code&gt; mode in IPython. Graphics are displayed inside the IPython-Notebook itself .For an introduction to IPython in general, visit the &lt;a href="http://ipython.org/"&gt;website&lt;/a&gt;. (R-Users: IPython is like a boosted, fast version of RStudio/knittr ;)) &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;pylab&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;Populating&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;interactive&lt;/span&gt; &lt;span class="n"&gt;namespace&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="nn"&gt;and&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Datetime parsing&lt;/h2&gt;
&lt;p&gt;To parse the datestrings in the csv-file, we need to write a little parser, using the standardlib's &lt;code&gt;datetime&lt;/code&gt; module. To inspect the documentation within the Notebook, use IPythons &lt;em&gt;?&lt;/em&gt;oOperator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;datetime?
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is the parser-function. It takes a string representation of a datetime and applies the conversion specified via the conversion string ` '%Y%m%d%H'.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def dateparser(datestring):
    return datetime.datetime.strptime(datestring,&amp;#39;%Y%m%d%H&amp;#39;)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Read the data&lt;/h2&gt;
&lt;p&gt;Using pandas excellent &lt;code&gt;read_&lt;/code&gt;-functions in combination with our parser, we can load the dataset within a single line of code. Furthermore, we should rename the columns (the original column-names include whitespace, and whitespace is sort of evil, when working with attributes in pandas).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;total= pd.read_csv(&amp;#39;total-volume.csv&amp;#39;,parse_dates=[0],date_parser=dateparser)
total.columns=[&amp;quot;date&amp;quot;,&amp;quot;tweets&amp;quot;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's have a look at the dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;total.head()
&lt;/pre&gt;&lt;/div&gt;


&lt;div style="max-height:1000px;max-width:1500px;overflow:auto;"&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;tweets&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2012-01-31 22:00:00&lt;/td&gt;
      &lt;td&gt;   33835&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2012-01-31 23:00:00&lt;/td&gt;
      &lt;td&gt; 1090096&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2012-02-01 00:00:00&lt;/td&gt;
      &lt;td&gt; 1096715&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2012-02-01 01:00:00&lt;/td&gt;
      &lt;td&gt; 1145446&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2012-02-01 02:00:00&lt;/td&gt;
      &lt;td&gt; 1114102&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;To work with the time-series, it's useful to set the &lt;em&gt;index&lt;/em&gt; (an index is, roughly spoken, the row-number in an Excel-Sheet) to the date-variable (One could have done this in the read_csv section):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;total.index = total.date
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Plot the data&lt;/h2&gt;
&lt;p&gt;Finally, plot the whole stuff with matplotlib. Adjust the size with the &lt;em&gt;rcParams&lt;/em&gt; or delete the &lt;em&gt;inline&lt;/em&gt; in the import section to plot outside of the IPython Notebook.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pylab.rcParams[&amp;#39;figure.figsize&amp;#39;] = (16.0, 8.0)
total.plot(marker=&amp;quot;o&amp;quot;,markerfacecolor=&amp;quot;red&amp;quot;)




&amp;lt;matplotlib.axes.AxesSubplot at 0x104151550&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/Twitter_Diversity_14_1.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Resample the data&lt;/h2&gt;
&lt;p&gt;To resample the time-series data, f.e. summing up weekly each monday, use the pandas excellent resampling methods. Plot the results to inspect the data. Change titles, axis labels etc. via pylab-methods&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;perday = total.resample(&amp;quot;W-Mon&amp;quot;,how=&amp;quot;sum&amp;quot;).plot(kind=&amp;quot;bar&amp;quot;)
title(&amp;quot;Tweets per Week&amp;quot;)
xlabel(&amp;quot;Sum of Tweets per Week&amp;quot;)




&amp;lt;matplotlib.text.Text at 0x104205350&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/Twitter_Diversity_16_1.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Regress the data&lt;/h2&gt;
&lt;p&gt;A simple (and a bit useless, but it's just for the sake of example) OLS-Regression with the shifted tweets (lag: 1 Day) can be done quite easy. Users familiar with R will note the differences (&lt;em&gt;writing&lt;/em&gt; models in Python like &lt;em&gt;y~x+whatever&lt;/em&gt; is &lt;a href="http://mpastell.com/2013/04/19/python_regression/"&gt;possible&lt;/a&gt; as well). Specify the model and print a short summary:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;model = pd.ols(y=log(total.tweets[:400]), x=total.tweets[:400].shift(-1), intercept=True)

print model.summary



-------------------------Summary of Regression Analysis-------------------------

Formula: Y ~ &amp;lt;x&amp;gt; + &amp;lt;intercept&amp;gt;

Number of Observations:         399
Number of Degrees of Freedom:   2

R-squared:         0.3118
Adj R-squared:     0.3101

Rmse:              0.4055

F-stat (1, 397):   179.8771, p-value:     0.0000

Degrees of Freedom: model 1, resid 397

-----------------------Summary of Estimated Coefficients------------------------
      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%
--------------------------------------------------------------------------------
             x     0.0000     0.0000      13.41     0.0000     0.0000     0.0000
     intercept    12.6846     0.0876     144.72     0.0000    12.5128    12.8564
---------------------------------End of Summary---------------------------------
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And plot the fitted values (&lt;em&gt;blue&lt;/em&gt;) for the first 400 cases afterwards against the empirical observations from the dataset (&lt;em&gt;red&lt;/em&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fig=figure()
sub=fig.add_subplot(111)
plot(model.sm_ols.model.fit().fittedvalues,linewidth=0,marker=&amp;quot;o&amp;quot;)
plot(log(total.tweets[:400]),color=&amp;quot;red&amp;quot;,marker=&amp;quot;o&amp;quot;,linewidth=0)




[&amp;lt;matplotlib.lines.Line2D at 0x105ebea10&amp;gt;]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/Twitter_Diversity_20_1.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;t=total.tweets.tolist()
t1=total.tweets.shift(-1).tolist()
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Push the data to R&lt;/h2&gt;
&lt;p&gt;While IPython becomes more and more language-independent, the rmagic and cell-magic functions are absolutely terrific! Using the rpy2 interface (I hope seamless conversion from numpy to R will improve further), it's almost too easy to pass data to R an use R's huge statistical  library . Of course, &lt;em&gt;ggplot&lt;/em&gt; excels &lt;em&gt;matplotlib&lt;/em&gt; in many, many ways. Let' plot the Tweets vs "lagged" Tweets and fit a simple OLS-Regression (see example above): &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nf"&gt;%load_ext&lt;/span&gt; &lt;span class="n"&gt;rmagic&lt;/span&gt;
&lt;span class="n"&gt;tweettoday&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tweets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tweetyesterday&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="n"&gt;total&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tweets&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;rmagic&lt;/span&gt; &lt;span class="n"&gt;extension&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;already&lt;/span&gt; &lt;span class="n"&gt;loaded&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;To&lt;/span&gt; &lt;span class="n"&gt;reload&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;use&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="nf"&gt;%reload_ext&lt;/span&gt; &lt;span class="n"&gt;rmagic&lt;/span&gt;



&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;tweettoday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tweetyesterday&lt;/span&gt;
&lt;span class="n"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ggplot2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;qplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tweettoday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tweetyesterday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tweettoday&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;tweetyesterday&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;



&lt;span class="nl"&gt;Call&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweettoday&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;tweetyesterday&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nl"&gt;Residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
     &lt;span class="n"&gt;Min&lt;/span&gt;       &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;   &lt;span class="n"&gt;Median&lt;/span&gt;       &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;      &lt;span class="n"&gt;Max&lt;/span&gt; 
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1314088&lt;/span&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;68943&lt;/span&gt;     &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;131&lt;/span&gt;    &lt;span class="mi"&gt;61755&lt;/span&gt;  &lt;span class="mi"&gt;1018282&lt;/span&gt;

&lt;span class="nl"&gt;Coefficients&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;Estimate&lt;/span&gt; &lt;span class="n"&gt;Std&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;Pr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Intercept&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="mf"&gt;8.663e+04&lt;/span&gt;  &lt;span class="mf"&gt;7.490e+03&lt;/span&gt;   &lt;span class="mf"&gt;11.57&lt;/span&gt;   &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mf"&gt;2e-16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="n"&gt;tweetyesterday&lt;/span&gt; &lt;span class="mf"&gt;9.211e-01&lt;/span&gt;  &lt;span class="mf"&gt;6.566e-03&lt;/span&gt;  &lt;span class="mf"&gt;140.28&lt;/span&gt;   &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mf"&gt;2e-16&lt;/span&gt; &lt;span class="o"&gt;***&lt;/span&gt;
&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nl"&gt;codes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;***&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt; &lt;span class="err"&gt;’&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;standard&lt;/span&gt; &lt;span class="nl"&gt;error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;127600&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;3466&lt;/span&gt; &lt;span class="n"&gt;degrees&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;freedom&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;observation&lt;/span&gt; &lt;span class="n"&gt;deleted&lt;/span&gt; &lt;span class="n"&gt;due&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;missingness&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Multiple&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nl"&gt;squared&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.8503&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="n"&gt;Adjusted&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nl"&gt;squared&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="mf"&gt;0.8502&lt;/span&gt; 
&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nl"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.968e+04&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;3466&lt;/span&gt; &lt;span class="n"&gt;DF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nl"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;2.2e-16&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="png" src="/images/Twitter_Diversity_24_1.png" /&gt;&lt;/p&gt;</summary><category term="Python"></category><category term="Pandas"></category><category term="Twitter"></category><category term="Reproducible"></category></entry><entry><title>User-Agents and URL-Resolvers</title><link href="/user-agents-and-url-resolvers.html" rel="alternate"></link><updated>2013-12-03T00:00:00+01:00</updated><author><name>Till Keyling</name></author><id>tag:,2013-12-03:user-agents-and-url-resolvers.html</id><summary type="html">&lt;p&gt;While resolving the (usually shortened) URL's from Tweets, I noticed some
errors using Python &lt;a href="" title="http://docs.python-requests.org/en/latest/"&gt;Requests&lt;/a&gt;.
Actually, it's not a Requests "Error", but a 403 ("Forbidden")-Response from
some Webservers, who don't like user-agents like Apache or the standard
Requests-User-Agent.&lt;/p&gt;
&lt;p&gt;Consider the follwing example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;rq&lt;/span&gt;

&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;user_agent = {&amp;#39;User-agent&amp;#39;: &amp;#39;Mozilla/5.0&amp;#39;}&amp;quot;&lt;/span&gt; &lt;span class="c"&gt;#This URL has some redirects,&lt;/span&gt;
&lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;doesn&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t like some User-Agents.&lt;/span&gt;

&lt;span class="c"&gt;#Lets try a simple head-request (which is recommended resolving the url due to&lt;/span&gt;
&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;drastically&lt;/span&gt; &lt;span class="c"&gt;#lower network-load, especially when resolving thousands of urls&lt;/span&gt;
&lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="n"&gt;once&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;exmpl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This example will result in an 403-Response code, mainly because of the
standard-headers that Requests is delivering:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="s"&gt;&amp;#39;User-Agent&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;python-requests/1.2.3 CPython/2.7.3 Linux/3.2.0-23-generic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Other
&lt;a href="" title="http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"&gt;Response-Codes&lt;/a&gt; may
be thrown, f.e. 405 when a head-Request is forbidden. This may be handled via
Exceptions, or using Curl ord PyCurl directly (which isn't under active
development any more). &lt;/p&gt;
&lt;p&gt;As a workaround, which "fakes" the user-agent of the request and should work
for the most webservers or endpoints of an request, one could &amp;amp; should simply
pass another user-agent (f.e. user_agent = {'User-agent': 'Mozilla/5.0'}). This
methods allows to resolve most of the shortened URL's posted on Twitter.&lt;/p&gt;
&lt;p&gt;To reduce the number of redirects when consuming Tweets from the Streaming API,
one should use the "expanded_url"-Value of the delivered JSON. Beware of the
fact that these "expansion" is not complete: Redirects, shortened Short-URL's
and alike are not expanded by that method!&lt;/p&gt;</summary></entry><entry><title>YouTube-Kanäle von deutschen Parteien</title><link href="/youtube-kanale-von-deutschen-parteien.html" rel="alternate"></link><updated>2013-11-27T00:00:00+01:00</updated><author><name>Till Keyling</name></author><id>tag:,2013-11-27:youtube-kanale-von-deutschen-parteien.html</id><summary type="html">&lt;p&gt;Im Rahmen der Erfassung der Bundestagswahlen auf YouTube durch das &lt;a href="http://www.fgpk.de/teilprojekte/#Teilprojekt1"&gt;Teilprojekt1&lt;/a&gt; speichern wir auch die Kanäle der 38 zur Bundestagswahl zugelassen Parteien, wovon ja letztlich nur &lt;a href="http://www.bundeswahlleiter.de/de/bundestagswahlen/BTW_BUND_13/presse/W13011_Wahlteilnahme_Parteien.html"&gt;34 Parteien&lt;/a&gt; teilnehmen werden. &lt;/p&gt;
&lt;p&gt;Zum Teil schwierig gestaltet sich dabei eine genaue Zuordnung der Kanäle von Parteien, insbesondere der Kleinparteien. Hier existieren unzählige Kanäle von Parteiverbänden auf Landesebene, von einzelnen Politikern (wobei wir ebenfalls die YouTube-Kanäle aller MdB abfragen). Bei den Volksparteien haben wir zudem noch die Kanäle der jeweiligen Fraktion im Bundestag aufgenommen. Gerade die Plattform &lt;a href="http://www.pluragraph.de"&gt;pluragraph.de&lt;/a&gt; ist bei der Erhebung solcher Daten sehr hilfreich (auch wenn eine eigene API a'la OffenesParlament hilfreich wäre), eine eigene Recherche auf dein jeweiligen Homepages von Parteien und Politiker aber leider unumgänglich. &lt;/p&gt;
&lt;h3&gt;Klickzahlen von YouTube-Kanälen&lt;/h3&gt;
&lt;p&gt;Eine Auflistung der YouTube-Nutzung nach Fraktionen findet sich ja bereits unter &lt;a href="http://www.hamburger-wahlbeobachter.de/2013/07/social-media-in-der-politik-90-prozent.html"&gt;hamburg-wahlbeobachter.de&lt;/a&gt;, wobei hier die Existenz eines eigenen Kanals noch längst keine Nutzung, geschweige denn Reichweite garantiert. Daher haben wir uns zunächst die Kanäle der Volksparteien auf YouTube (und deren Fraktionen) näher angeschaut und deren Entwicklung im letzten Monat visualisiert.&lt;/p&gt;
&lt;h4&gt;CDU/CSU-Fraktionskanal&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;cducsu&lt;/em&gt; besteht seit 2006-10-30 und erreicht mit seinen &lt;strong&gt;169 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;145.724 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 52.887-mal besucht, 378 User haben den Kanal abonniert und 2-mal favorisiert. 
Das letzte Update fand am 2013-07-30 statt.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der CDU/CSU-Fraktion im Bundestag" src="/images/cducsu.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;CSUMedia (CSU)&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;csumedia&lt;/em&gt; besteht seit 2008-02-11 und erreicht mit seinen &lt;strong&gt;274 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;919.616 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 157.006-mal besucht, 559 User haben den Kanal abonniert und 1-mal favorisiert. 
Das letzte Update fand am 2013-08-08 statt.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der CSU" src="/images/csumedia.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;CDU-TV (CDU)&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;cdutv&lt;/em&gt; besteht seit 2008-08-05 und erreicht mit seinen &lt;strong&gt;662 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;2.403.688 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 796.529-mal besucht, 3205 User haben den Kanal abonniert und 4-mal favorisiert. 
Das letzte Update fand am 2013-08-07 statt.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der CDU" src="/images/cdutv.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;Linksfraktion (Die Linke)&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;linksfraktion&lt;/em&gt; besteht seit 2008-01-05 und erreicht mit seinen &lt;strong&gt;2.250 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;6.705.403 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 1.510.743-mal besucht, 9.352 User haben den Kanal abonniert und 13-mal favorisiert. 
Das letzte Update fand am 2013-08-07 statt&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der Linksfraktion" src="/images/linksfraktion.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;Die Linke&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;dielinke&lt;/em&gt; besteht seit 2006-09-20 und erreicht mit seinen &lt;strong&gt;533 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;1.352.618 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 284.492-mal besucht, 4.437 User haben den Kanal abonniert und 20-mal favorisiert. 
Das letzte Update fand am 2013-08-05 statt&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der Linken" src="/images/dielinke.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;SPDVision (SPD)&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;spdvision&lt;/em&gt; besteht seit 2007-10-17 und erreicht mit seinen &lt;strong&gt;688 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;2.414.547 Klicks&lt;/strong&gt;. 
 Die Kanalhomepage selbst wurde bisher 540.936-mal besucht, 3.836 User haben den Kanal abonniert und 7-mal favorisiert. 
 Das letzte Update fand am 2013-08-09 statt&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der SPDVisione" src="/images/spdvision.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;SPDFraktion (SPD)&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;spdfraktion&lt;/em&gt; besteht seit 2009-09-17 und erreicht mit seinen &lt;strong&gt;263 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;1.629.82 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 28.827-mal besucht, 541 User haben den Kanal abonniert und 0-mal favorisiert. 
Das letzte Update fand am 2013-07-30 statt&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der SPD-fraktion" src="/images/spdfraktion.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;FDP&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;fdp&lt;/em&gt; besteht seit 2006-02-16 und erreicht mit seinen &lt;strong&gt;1186 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;2.877.713 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 1.374.048-mal besucht, 2.628 User haben den Kanal abonniert und 11-mal favorisiert. 
Das letzte Update fand am 2013-08-04 statt&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der FDP-Fraktion" src="/images/fdp.svg" /&gt;&lt;/p&gt;
&lt;h4&gt;FDPPArtei (FDP)&lt;/h4&gt;
&lt;p&gt;Der Kanal &lt;em&gt;fdppartei&lt;/em&gt; besteht seit 2009-09-02 und erreicht mit seinen &lt;strong&gt;4 Videos&lt;/strong&gt; insgesamt &lt;strong&gt;1243 Klicks&lt;/strong&gt;. 
Die Kanalhomepage selbst wurde bisher 14.074-mal besucht, 190 User haben den Kanal abonniert und 1-mal favorisiert. 
Das letzte Update fand am 2013-07-30 statt&lt;/p&gt;
&lt;p&gt;&lt;img alt="Kanal der FDP" src="/images/fdppartei.svg" /&gt;&lt;/p&gt;
&lt;h3&gt;Übersicht über die Parteikanäle aller zur Bundestagswahl zugelassenen Parteien&lt;/h3&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;URLOFTHISPOST&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c"&gt;#erste und einzige Tabelle als Datensatz&lt;/span&gt;

&lt;span class="c"&gt;#Views plotten; matplotlib etc. vorrausgesetzt&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c"&gt;#etc...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Username&lt;/th&gt;
&lt;th&gt;Seit&lt;/th&gt;
&lt;th&gt;Videos&lt;/th&gt;
&lt;th&gt;Video-Views kumuliert&lt;/th&gt;
&lt;th&gt;Channel-Views&lt;/th&gt;
&lt;th&gt;Subscriber&lt;/th&gt;
&lt;th&gt;Favorites&lt;/th&gt;
&lt;th&gt;Updated&lt;/th&gt;
&lt;th&gt;Views/Video&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;linksfraktion&lt;/td&gt;
&lt;td&gt;2008-01-05 12:12:08&lt;/td&gt;
&lt;td&gt;2250&lt;/td&gt;
&lt;td&gt;6705403&lt;/td&gt;
&lt;td&gt;1510743&lt;/td&gt;
&lt;td&gt;9352&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;2013-08-07 09:26:16&lt;/td&gt;
&lt;td&gt;2980&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spdvision&lt;/td&gt;
&lt;td&gt;2007-10-17 20:42:17&lt;/td&gt;
&lt;td&gt;688&lt;/td&gt;
&lt;td&gt;2414547&lt;/td&gt;
&lt;td&gt;540936&lt;/td&gt;
&lt;td&gt;3836&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2013-08-09 07:58:28&lt;/td&gt;
&lt;td&gt;3509&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;buesojugend&lt;/td&gt;
&lt;td&gt;2008-07-23 13:46:30&lt;/td&gt;
&lt;td&gt;852&lt;/td&gt;
&lt;td&gt;1342025&lt;/td&gt;
&lt;td&gt;92498&lt;/td&gt;
&lt;td&gt;2342&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;2013-08-08 18:55:09&lt;/td&gt;
&lt;td&gt;1575&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dielinke&lt;/td&gt;
&lt;td&gt;2006-09-20 09:23:15&lt;/td&gt;
&lt;td&gt;533&lt;/td&gt;
&lt;td&gt;1352618&lt;/td&gt;
&lt;td&gt;284492&lt;/td&gt;
&lt;td&gt;4437&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;2013-08-05 12:06:02&lt;/td&gt;
&lt;td&gt;2537&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;csumedia&lt;/td&gt;
&lt;td&gt;2008-02-11 15:03:14&lt;/td&gt;
&lt;td&gt;274&lt;/td&gt;
&lt;td&gt;919616&lt;/td&gt;
&lt;td&gt;157006&lt;/td&gt;
&lt;td&gt;559&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2013-08-08 06:56:48&lt;/td&gt;
&lt;td&gt;3356&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cdutv&lt;/td&gt;
&lt;td&gt;2008-08-05 14:51:56&lt;/td&gt;
&lt;td&gt;662&lt;/td&gt;
&lt;td&gt;2403688&lt;/td&gt;
&lt;td&gt;796529&lt;/td&gt;
&lt;td&gt;3205&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2013-08-07 12:28:26&lt;/td&gt;
&lt;td&gt;3630&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;videoprodeutschland&lt;/td&gt;
&lt;td&gt;2010-07-01 20:27:17&lt;/td&gt;
&lt;td&gt;51&lt;/td&gt;
&lt;td&gt;110032&lt;/td&gt;
&lt;td&gt;16814&lt;/td&gt;
&lt;td&gt;257&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-07-30 00:19:59&lt;/td&gt;
&lt;td&gt;2157&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;piratenpartei&lt;/td&gt;
&lt;td&gt;2007-03-13 23:03:57&lt;/td&gt;
&lt;td&gt;144&lt;/td&gt;
&lt;td&gt;356198&lt;/td&gt;
&lt;td&gt;178778&lt;/td&gt;
&lt;td&gt;4979&lt;/td&gt;
&lt;td&gt;4220&lt;/td&gt;
&lt;td&gt;2013-08-09 10:10:23&lt;/td&gt;
&lt;td&gt;2473&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gruene&lt;/td&gt;
&lt;td&gt;2006-05-25 14:01:08&lt;/td&gt;
&lt;td&gt;1324&lt;/td&gt;
&lt;td&gt;2929485&lt;/td&gt;
&lt;td&gt;369752&lt;/td&gt;
&lt;td&gt;3890&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;2013-08-09 08:28:04&lt;/td&gt;
&lt;td&gt;2212&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;spdfraktion&lt;/td&gt;
&lt;td&gt;2009-09-17 07:57:53&lt;/td&gt;
&lt;td&gt;263&lt;/td&gt;
&lt;td&gt;162982&lt;/td&gt;
&lt;td&gt;28827&lt;/td&gt;
&lt;td&gt;541&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-07-30 02:25:47&lt;/td&gt;
&lt;td&gt;619&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fdp&lt;/td&gt;
&lt;td&gt;2006-02-16 00:30:55&lt;/td&gt;
&lt;td&gt;1186&lt;/td&gt;
&lt;td&gt;2877713&lt;/td&gt;
&lt;td&gt;1374048&lt;/td&gt;
&lt;td&gt;2628&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;2013-08-04 15:57:27&lt;/td&gt;
&lt;td&gt;2426&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wahlalternative2013&lt;/td&gt;
&lt;td&gt;2012-09-25 18:59:25&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;99268&lt;/td&gt;
&lt;td&gt;14512&lt;/td&gt;
&lt;td&gt;997&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2013-08-07 15:33:20&lt;/td&gt;
&lt;td&gt;3970&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;alternativefuerde&lt;/td&gt;
&lt;td&gt;2013-03-19 00:44:14&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;101648&lt;/td&gt;
&lt;td&gt;444&lt;/td&gt;
&lt;td&gt;1124&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2013-07-30 01:30:05&lt;/td&gt;
&lt;td&gt;4065&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cducsu&lt;/td&gt;
&lt;td&gt;2006-10-30 13:52:47&lt;/td&gt;
&lt;td&gt;169&lt;/td&gt;
&lt;td&gt;145724&lt;/td&gt;
&lt;td&gt;52887&lt;/td&gt;
&lt;td&gt;378&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2013-07-30 01:04:55&lt;/td&gt;
&lt;td&gt;862&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;fdppartei&lt;/td&gt;
&lt;td&gt;2009-09-02 16:36:19&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1243&lt;/td&gt;
&lt;td&gt;14074&lt;/td&gt;
&lt;td&gt;190&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2013-07-30 01:37:05&lt;/td&gt;
&lt;td&gt;310&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;neinidee&lt;/td&gt;
&lt;td&gt;2012-04-06 16:12:04&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;9252&lt;/td&gt;
&lt;td&gt;1124&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2013-07-30 01:15:37&lt;/td&gt;
&lt;td&gt;616&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0sul&lt;/td&gt;
&lt;td&gt;2007-04-09 19:17:21&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;154565&lt;/td&gt;
&lt;td&gt;6578&lt;/td&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;117&lt;/td&gt;
&lt;td&gt;2013-07-30 01:58:23&lt;/td&gt;
&lt;td&gt;2972&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mlpdde&lt;/td&gt;
&lt;td&gt;2009-05-28 17:57:30&lt;/td&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;td&gt;72335&lt;/td&gt;
&lt;td&gt;36050&lt;/td&gt;
&lt;td&gt;367&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2013-07-30 02:13:10&lt;/td&gt;
&lt;td&gt;1315&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bigdeutschland&lt;/td&gt;
&lt;td&gt;2010-03-31 09:09:03&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;44356&lt;/td&gt;
&lt;td&gt;2215&lt;/td&gt;
&lt;td&gt;32&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-06-26 06:25:59&lt;/td&gt;
&lt;td&gt;4032&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;oedptv&lt;/td&gt;
&lt;td&gt;2011-05-31 17:00:48&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;3086&lt;/td&gt;
&lt;td&gt;837&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-08-07 20:58:59&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;republikanertv&lt;/td&gt;
&lt;td&gt;2008-01-07 00:44:51&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;td&gt;341072&lt;/td&gt;
&lt;td&gt;140993&lt;/td&gt;
&lt;td&gt;340&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;2013-07-30 01:50:28&lt;/td&gt;
&lt;td&gt;2795&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kommunistentv&lt;/td&gt;
&lt;td&gt;2009-02-02 11:56:32&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;21031&lt;/td&gt;
&lt;td&gt;51377&lt;/td&gt;
&lt;td&gt;484&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-07-30 01:46:50&lt;/td&gt;
&lt;td&gt;2103&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pbcbuero&lt;/td&gt;
&lt;td&gt;2009-05-07 18:38:01&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;84040&lt;/td&gt;
&lt;td&gt;2495&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;2013-07-30 02:49:19&lt;/td&gt;
&lt;td&gt;28013&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bayernparteitv&lt;/td&gt;
&lt;td&gt;2009-08-16 13:32:40&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;40317&lt;/td&gt;
&lt;td&gt;13595&lt;/td&gt;
&lt;td&gt;117&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-07-30 00:57:05&lt;/td&gt;
&lt;td&gt;1390&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dietierschutzpartei&lt;/td&gt;
&lt;td&gt;2009-03-24 11:08:28&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;44851&lt;/td&gt;
&lt;td&gt;15384&lt;/td&gt;
&lt;td&gt;197&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;2013-07-30 00:33:53&lt;/td&gt;
&lt;td&gt;6407&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;dieparteidervernunft&lt;/td&gt;
&lt;td&gt;2011-05-18 20:30:04&lt;/td&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td&gt;341738&lt;/td&gt;
&lt;td&gt;34847&lt;/td&gt;
&lt;td&gt;1293&lt;/td&gt;
&lt;td&gt;83&lt;/td&gt;
&lt;td&gt;2013-08-03 23:06:55&lt;/td&gt;
&lt;td&gt;3839&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;violettenpresse&lt;/td&gt;
&lt;td&gt;2009-08-23 13:00:26&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;65215&lt;/td&gt;
&lt;td&gt;842&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2013-06-26 15:12:35&lt;/td&gt;
&lt;td&gt;32607&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;familienwahl&lt;/td&gt;
&lt;td&gt;2009-01-31 03:32:33&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;15946&lt;/td&gt;
&lt;td&gt;551&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2013-06-26 15:29:39&lt;/td&gt;
&lt;td&gt;1328&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;familienpartei&lt;/td&gt;
&lt;td&gt;2007-12-02 08:05:49&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1988&lt;/td&gt;
&lt;td&gt;676&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2013-06-22 11:24:21&lt;/td&gt;
&lt;td&gt;662&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;christlichemitte&lt;/td&gt;
&lt;td&gt;2011-06-21 15:13:40&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;1515&lt;/td&gt;
&lt;td&gt;603&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2013-07-30 03:02:59&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;gleichheittv&lt;/td&gt;
&lt;td&gt;2009-04-08 09:57:44&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;48727&lt;/td&gt;
&lt;td&gt;8629&lt;/td&gt;
&lt;td&gt;69&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;2013-07-30 02:09:13&lt;/td&gt;
&lt;td&gt;2214&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;neuemitte&lt;/td&gt;
&lt;td&gt;2009-12-29 17:49:32&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;24316&lt;/td&gt;
&lt;td&gt;3563&lt;/td&gt;
&lt;td&gt;711&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2013-08-07 19:54:55&lt;/td&gt;
&lt;td&gt;3039&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</summary><category term="YouTube"></category><category term="Bundestagswahl"></category><category term="DFG"></category></entry><entry><title>Sampling YouTube-Clips from Twitter</title><link href="/sampling-youtube-clips-from-twitter.html" rel="alternate"></link><updated>2012-11-27T00:00:00+01:00</updated><author><name>Till Keyling</name></author><id>tag:,2012-11-27:sampling-youtube-clips-from-twitter.html</id><summary type="html">&lt;p&gt;During the  electoral campaigning for the recent german federal elections in
2013, we decided not just to monitor YouTube-Clips via &lt;em&gt;classical&lt;/em&gt; sampling
techniques like keyword-searches on YouTube. Therefore, we took advantage of the
&lt;a href="http://arxiv.org/abs/1301.6932"&gt;cross-pollinated&lt;/a&gt; diffusion processes between
Social Network Sites (SNS) like Facebook or - in this case - Twitter:&lt;/p&gt;
&lt;p&gt;To gain an understanding of which YouTube-Clips are relevant in terms of
political participation and discussion on Twitter, we collected Tweets which are
related to the federal elections, resolved the URLS within those Tweets and - if
an URL linked to a YouTube-Clip - included these Clips in our YouTube-Election
sample. This method enables the &lt;em&gt;early-bird&lt;/em&gt; detection of (likely) political
YouTube-Clips and supplements the more traditional data-collection process via
YouTube search terms, channel monitoring or crawling the YouTube-Network of
related videoclips&lt;/p&gt;
&lt;h2&gt;Collecting political Tweets via Streaming-API&lt;/h2&gt;
&lt;p&gt;Twitter, as opposed to Facebook as a less accessible SNS in terms of crawling
topics, posts and alike, became sort of a favourite toy for the social
science/information science etc. To a very large extend, Twitter (comparetivly)
open API's and a vast collection of tool/websites to get data out of the system
may account for this affinity towards the quite small SNS - when compared to
Facebook. However, although Twitter-Users might be completely un-representative,
there is a potential of Twitter as an early-warning system, ifnluential intra-
media-agenda-setting authority. When dealing with the dissemination of
(political) content,
&lt;a href="https://speakerdeck.com/dorvak/news-diffusion-ecrea-2012"&gt;we&lt;/a&gt; can
show that news-articles disseminate lamost an order of magnitude faster on
Twitter than on Facebook.&lt;/p&gt;
&lt;script async class="speakerdeck-embed" data-id="3e041690b8e401310db522637f6c77d3" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"&gt;&lt;/script&gt;

&lt;p&gt;Therefore, we conclude that Twitter is sort of a news-ticker, while Facebook is
used for the discussion of links and content within smaller &lt;a href="http://www.schmidtmitdete.de/english-content"&gt;'personal
publics'&lt;/a&gt;. This holds true for the
diffusion of audiovisual-content as well: Tweets for YouTube-Clips usually
precede the Shares, Likes and Comments on Facebook (see one example above).&lt;/p&gt;
&lt;p&gt;What´s crucial here and should not be underestimated is the &lt;em&gt;flow of content&lt;/em&gt;
not only &lt;em&gt;within&lt;/em&gt; SNS, but &lt;em&gt;across&lt;/em&gt; different online-outlets. These &lt;em&gt;cross-
pollination&lt;/em&gt; process might be triggered by users who uses both multiple networks
and once (Studies?) and disseminate content/links  - like bee's transport pollen
- into other networks. Of course, users who do create content on platforms like
YouTube tend to use Twitter and Facebook (Study) and therefore &lt;em&gt;pollute&lt;/em&gt; other
SNS intentionally (which links to the concept of virality ..). From a more
abstract, system-theoretical viewpoint, social or communicative subsystems like
Twitter and Facebook observe and react to each other (again, Twitter is more
observable than Facebook, f.e.).&lt;/p&gt;</summary></entry><entry><title>YouTube Language-Detection</title><link href="/youtube-language-detection.html" rel="alternate"></link><updated>2012-11-27T00:00:00+01:00</updated><author><name>Till Keyling</name></author><id>tag:,2012-11-27:youtube-language-detection.html</id><summary type="html">&lt;p&gt;Due to the lack of a language-tag in the YouTube-API, which would be
extremely helpful determining the language of a videoclip (precisely:
the language of the title and description; language detection in
audiovisual content requires much more effort and cpu-power), i ran a
quick test with python´s &lt;a href="https://bitbucket.org/spirit/guess_language/"&gt;guess-language&lt;/a&gt; module
I think it´s a simple but pretty reliable algorithm for language
detection tasks, based on a trigram detection and the enchant-library
for over 60 languages.&lt;/p&gt;
&lt;p&gt;In our project about political communication on &lt;a href="http://www.fgpk.de/en/teilprojekte/#Teilprojekt1"&gt;YouTube&lt;/a&gt;
we stumbled upon a large amount of non-german videoclips (sampling from
the feed for the most-viewed videos in the “news &amp;amp; politics” - category
in germany). To get an overview of the linguistic diversity on the
german version on YouTube, I conducted a language-detection test on a
small subsample of our data (350 videoclips).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://dl.dropbox.com/u/20490817/lang_barplot.png"&gt;&lt;img alt="image" src="https://dl.dropbox.com/u/20490817/lang_barplot.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;First of all, just about 50% of the recent videos (“top viewed in
news..”) have a german description (30 of them have a title in another
language; a detection solely based on the (usually short) titles seems
to be unreliable), followed by clips with an english-speaking
description. The amount of an unsuccessful classification task
(“UNKNOWN”) is noteworthy, but may be reduced by a combination with the
title-classification. In line with the results of our manual-coding
approach, a significant amount of descriptions and titles are in an
arabic language. Furthermore, the results of the automatic
language-detection could be combined with videoclip-metadata.
&lt;a href="https://dl.dropbox.com/u/20490817/lang_jitter.png"&gt;&lt;img alt="image" src="https://dl.dropbox.com/u/20490817/lang_jitter.png" /&gt;&lt;/a&gt;
For example, videoclips with a german description are obviously less
successful than those with an arabic or english description. At the
moment, i´m validating the classification results with a manual coding
of the titles and descriptions, but this approach might be a starting
point to reduce/filter the sample-size of audiovisual material, esp. in
preparation of a manual content analysis or simply to get an overview of
a huge amount of user-generated-content).&lt;/p&gt;</summary><category term="YouTube"></category><category term="R"></category><category term="Python"></category></entry><entry><title>Crowdsourcing und Sampling auf Twitter</title><link href="/crowdsourcing-und-sampling-auf-twitter.html" rel="alternate"></link><updated>2012-07-23T00:00:00+02:00</updated><author><name>Till Keyling</name></author><id>tag:,2012-07-23:crowdsourcing-und-sampling-auf-twitter.html</id><summary type="html">&lt;p&gt;Im Rahmen unseres Forschungsprojekts “Politische Kommunikation auf
Videoplattformen” erfassen wir auch Videos zur Bundestagswahl 2013.
Nicht jetzt erst merken wir, dass die Stichprobenbildung auf
“Folksonomy”-strukturierten Angeboten wie YouTube ein zentrales Problem
bleibt, vermutlich sogar eine DER methodischen Herausforderungen im
Bereich der &lt;a href="http://www.univie.ac.at/digitalmethods/"&gt;digital methods&lt;/a&gt;
bleiben wird, weil die Content-Anbieter zwar Zugänge via API anbieten,
aber oft &lt;a href="https://code.google.com/p/gdata-issues/issues/detail?id=4113"&gt;keine
Zufallsstichproben&lt;/a&gt;
erlauben (etc. pp.).&lt;/p&gt;
&lt;p&gt;Zunächst einmal gibt es ganz offensichtliche und systematische Zugänge
der Stichprobenbildung im politischen Bereich, wobei gerade die
thematische Zentrierung auf die Bundestagswahlen noch einen Glücksfall
darstellen:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Akteurs- oder Channel-basiertes Sampling: In diesem Fall erfassen
    wir die öffentlichen YouTube-Kanäle der aktuellen Mitglieder des
    Bundestages und der großen Volksparteien, die vorher manuell erfasst
    wurden.&lt;/li&gt;
&lt;li&gt;Keyword-basiertes Sampling, indem nach Stichworten über die
    YouTube-Suche Videos zum Thema Bundestagswahlen erfasst werden.
    Zudem können Tweets erfasst werden, die bestimmte Stichworte in
    Kombination mit Video-Links enthalten, also ein Sampling über die
    Beobachtung von Anschlusskommunikation auf anderen Netzwerken,
    gerade weil diese “cross-pollination”, d.h. die Diffusion von
    Informationen über SNS-Grenzen hinweg immer wichtiger wird, gerade
    für Content-Plattformen wie YouTube (das werte ich unter anderem in
    meiner Dissertation aus).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Heute ist mir aber noch eine andere Idee gekommen, Stichproben bilden zu
lassen, und zwar per crowd-sourcing, aber im Wortsinn: Nutzer liefern
gezielt Informationen in eine Datenbank oder konstituieren eine
Stichprobe, wobei die Auswahl nach den Relevanzkriterien der Nutzer
erfolgt - damit aber auch bewusstem Missbrauch etc. Tür und Tor geöffnet
werden:&lt;/p&gt;
&lt;p&gt;Unter &lt;a href="http://twitter.com/btwtube" title="BTW Tube"&gt;@btwtube&lt;/a&gt; habe ich einen
Bot-Account eingerichtet, an den Nutzer Vorschläge für relevante
YouTube-Videos liefern können, die den aktuellen Status der
Bundestagswahl kommentieren. Setzt jemand einen Tweet an @btwtube oder
mit dem hashtag #btwtube ab, werden die im Tweet enthaltenen URL´s
geparsed und der Stichprobe hinzugefügt, so sie YouTube-Videos
darstellen. Artig bedankt sich das Programm für die eingereichten
Videos, auch wenn das Dummerchen (noch) nicht zwischen relevanten und
irrelevanten Videoclips differenzieren kann (mit jedem eingehenden Video
wird allerdings ein Stromstoß an einen Hiwi abgesetzt, der diese dann zu
katalogisieren hat).&lt;/p&gt;
&lt;p&gt;&lt;a class="twitter-timeline" href="https://twitter.com/btwtube" data-widget-id="646244807480901632"&gt;Tweets by @btwtube&lt;/a&gt;
&lt;script&gt;!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Soweit zur Logik. Natürlich ist das ganze hochgradig experimentell,
davon abhängig, dass Nutzer aktiv werden und Videos vorschlagen, um so
die Stichprobe zu erweitern. Mir schweben da noch einige
Anwendungsbereiche vor, in denen sich eine solche Art der
Stichprobenbildung sinnvoller einsetzen lässt, um jegliche Anmerkungen
bin ich aber auch dankbar. Im übrigen werden ich die Ergebnisse und die
aktuellen Trends zur Bundestagswahl auf YouTube auch möglichst zeitnah
auf einer Live-Website zur Verfügung stellen, wenn dafür noch Zeit ist.&lt;/p&gt;</summary><category term="YouTube"></category><category term="Twitter"></category><category term="Bots"></category></entry></feed>